{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransAdv.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO7VDbHDZ8pkSWbmbboYu+5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2zVwSV9A52HI"},"source":["# Transferability of Adversarial Examples\n","This is a small experiment relating to the trasferability of adversarial examples, here we go."]},{"cell_type":"markdown","metadata":{"id":"6hixdTLi-sFg"},"source":["## Import Essential Packages"]},{"cell_type":"code","metadata":{"id":"vx2dyzXu5P3Y"},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import keras.layers as layers\n","import numpy as np\n","\n","import os\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1bctZQFFelf"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BoRPluIOYL1Y"},"source":["As the imagenet pretrained model of tensorflow can't be trained in a shor time, I implement it with **pytorch** whose cifar-10 pretrained model can be downloaded from github."]},{"cell_type":"code","metadata":{"id":"G4v2uOkyXQN9","executionInfo":{"status":"ok","timestamp":1602578091587,"user_tz":-480,"elapsed":47684,"user":{"displayName":"谢斯棋","photoUrl":"","userId":"13460657872157698438"}},"outputId":"399a7226-0d75-408e-da3c-ab8b80d59971","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["drive.mount(\"/content/drive\")\n","path = \"/content/drive/My Drive/adv\"\n","\n","os.chdir(path)\n","os.listdir(path)\n","\n","from cifar10_models import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EmSQuTLs5gVU"},"source":["## Define Constants\n","Constants like batch size."]},{"cell_type":"code","metadata":{"id":"pCNtUQdY5fsk"},"source":["batch_size = 64\n","epochs = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shfyRP3S_Dxs"},"source":["## Import Dataset\n","Experiment on on CIFAR-10 implemented with tensorflow."]},{"cell_type":"code","metadata":{"id":"qVOPKKcI_E_4","executionInfo":{"status":"ok","timestamp":1602050288186,"user_tz":-480,"elapsed":11115,"user":{"displayName":"谢斯棋","photoUrl":"","userId":"13460657872157698438"}},"outputId":"da4524e1-6778-4719-c013-0f1e73304727","colab":{"base_uri":"https://localhost:8080/"}},"source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 6s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yoThJM4lPNGl"},"source":["y_train = tf.one_hot(indices=y_train, depth=10)\n","y_test = tf.one_hot(indices=y_test, depth=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tw4l7FyXp06i"},"source":["print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8W2rT1TQZeXf"},"source":["As tensorflow didn't work quite well, another trial has been done in **pytorch**."]},{"cell_type":"code","metadata":{"id":"ezqnyccGZe0F","executionInfo":{"status":"ok","timestamp":1602579592925,"user_tz":-480,"elapsed":11989,"user":{"displayName":"谢斯棋","photoUrl":"","userId":"13460657872157698438"}},"outputId":"8395d5aa-7e5d-4c96-fd6b-3f69cbd1550c","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2xz5ZOAA_ADo"},"source":["## Define Models\n","Keras has included some model structures, so I directly use them as the first try. And I will try VGG-16, ResNet-50 and Inception v3."]},{"cell_type":"code","metadata":{"id":"3Cv3CRgb-_jc"},"source":["# VGG16\n","vgg16_base = tf.keras.applications.VGG16(\n","    include_top=False, weights='imagenet', input_tensor=None, input_shape=[32,32,3],\n","    pooling=max, classifier_activation='softmax'\n",")\n","\n","x = vgg16_base.output\n","x = layers.Flatten()(x)\n","x = layers.Dense(4096,activation='relu')(x)\n","x = layers.Dense(4096,activation='relu')(x)\n","x = layers.Dense(10, activation='softmax')(x)\n","\n","vgg16 = keras.Model(inputs=vgg16_base.input, outputs=x)\n","\n","# ResNet50\n","resnet50_base = tf.keras.applications.ResNet50(\n","    include_top=False, weights='imagenet', input_tensor=None, input_shape=[32,32,3],\n","    pooling=max\n",")\n","\n","x = resnet50_base.output\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(10, activation='softmax')(x)\n","\n","resnet50 = keras.Model(inputs=resnet50_base.input, outputs=x)\n","\n","# Inception v3\n","inceptionv3_base = tf.keras.applications.InceptionV3(\n","    include_top=False, weights='imagenet', input_tensor=None, input_shape=None,\n","    pooling=max, classifier_activation='softmax'\n",")\n","\n","x = inceptionv3_base.output\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(10, activation='softmax')(x)\n","\n","inceptionv3 = keras.Model(inputs=inceptionv3_base.input, outputs=x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUk2uzZxU3Gq"},"source":["vgg16.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZukECVtU_w4"},"source":["resnet50.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PlHiVhegVAHd"},"source":["inceptionv3_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PXzv-RjWvnC"},"source":["Implement with **pytorch** using pretrained models."]},{"cell_type":"code","metadata":{"id":"2Au2pA6-ilJw","executionInfo":{"status":"ok","timestamp":1602334064697,"user_tz":-480,"elapsed":170052,"user":{"displayName":"谢斯棋","photoUrl":"","userId":"13460657872157698438"}},"outputId":"5a2ab250-2698-401f-9c08-52324202d2d7","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!python cifar10_download.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100% 2.46G/2.46G [01:24<00:00, 29.0MMiB/s]\n","Download successful. Unzipping file.\n","Unzip file successful!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wROY9L1T2NsN"},"source":["del(vgg16)\n","del(resnet50)\n","del(inception_v3)\n","from cifar10_models import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKMkYupGWhmC"},"source":["vgg16 = vgg16_bn(pretrained=True)\n","\n","resnet50 = resnet50(pretrained=True)\n","\n","inception_v3 = inception_v3(pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyPWCKGqAyZ9"},"source":["## Train Models\n","Tran the models with standard data so as to make the models have abilities to classify the images, what's more, lead the models to learn the features of the dataset."]},{"cell_type":"code","metadata":{"id":"mV_BOq_YAzbM"},"source":["optimazer_vgg = tf.keras.optimizers.Adam(\n","    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True,\n","    name='Adam'\n",")\n","\n","optimazer_resnet = tf.keras.optimizers.Adam(\n","    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True,\n","    name='Adam'\n",")\n","\n","optimazer_inception = tf.keras.optimizers.Adam(\n","    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True,\n","    name='Adam'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZ0CNFo3a16x"},"source":["vgg16.compile(\n","    optimizer=optimazer_vgg, \n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","    metrics=['accuracy']\n",")\n","\n","resnet50.compile(\n","    optimizer=optimazer_resnet, \n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","    metrics=['accuracy']\n",")\n","\n","inceptionv3.compile(\n","    optimizer=optimazer_inception, \n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","    metrics=['accuracy']\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qb2_mazlaLiH"},"source":["vgg16.fit(x=x_train, y=y_train, validation_split =0.1, verbose=1, batch_size=batch_size, epochs=epochs)\n","\n","# resnet50.fit(x=x_train, y=y_train, validation_split=0.1, verbose=1, batch_size=batch_size, epochs=epochs)\n","\n","# inceptionv3.fit(x=x_train, y=y_train, validation_split=0.1, verbose=1, batch_size=batch_size, epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8NCF15fiSSN"},"source":["## Test Model\n","Test the accuracy of the models."]},{"cell_type":"code","metadata":{"id":"cazFOe4cb8CC"},"source":["device = torch.device(\"cuda\")\n","vgg16.to(device)\n","resnet50.to(device)\n","inception_v3.to(device)\n","vgg16.eval()\n","resnet50.eval()\n","inception_v3.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFoDj2dciR2L","executionInfo":{"status":"ok","timestamp":1602579717513,"user_tz":-480,"elapsed":96199,"user":{"displayName":"谢斯棋","photoUrl":"","userId":"13460657872157698438"}},"outputId":"c8238dcf-6659-4c88-bbaf-038456c3749f","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["correct_vgg = correct_resnet = correct_inception = 0\n","total = 0\n","with torch.no_grad():\n","  for data in testloader:\n","      images, labels = data\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs_vgg = vgg16(images)\n","      outputs_resnet = resnet50(images)\n","      outputs_inception = inception_v3(images)\n","      _, predicted_vgg = torch.max(outputs_vgg.data, 1)\n","      _, predicted_resnet = torch.max(outputs_resnet.data, 1)\n","      _, predicted_inception = torch.max(outputs_inception.data, 1)\n","      total += labels.size(0)\n","      correct_vgg += (predicted_vgg == labels).sum().item()\n","      correct_resnet += (predicted_resnet == labels).sum().item()\n","      correct_inception += (predicted_inception == labels).sum().item()\n","\n","\n","print('Accuracy of the vgg16 on the 10000 test images: %d%%' % (100 * correct_vgg / total))\n","print('Accuracy of the resnet50 on the 10000 test images: %d%%' % (100 * correct_resnet / total))\n","print('Accuracy of the inception_v3 on the 10000 test images: %d%%' % (100 * correct_inception / total))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the vgg16 on the 10000 test images: 92%\n","Accuracy of the resnet50 on the 10000 test images: 90%\n","Accuracy of the inception_v3 on the 10000 test images: 90%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RJl7Lih3kXKK"},"source":["## Adversarial Examples\n","Here I am going to generate some adversarial examples of one specific model structure, then apply them to other models to check the transibility of adversarial examples."]},{"cell_type":"code","metadata":{"id":"gpgj4IAW1c8H"},"source":["# FGSM attack code\n","def fgsm_attack(image, epsilon, data_grad):\n","  # Collect the element-wise sign of the data gradient\n","  sign_data_grad = data_grad.sign()\n","  # Create the perturbed image by adjusting each pixel of the input image\n","  perturbed_image = image + epsilon*sign_data_grad\n","  # Adding clipping to maintain [0,1] range\n","  # perturbed_image = torch.clamp(perturbed_image, 0, 1)\n","  # Return the perturbed image\n","  return perturbed_image\n","\n","\n","def test(target_model, other_model_1, other_model_2, device, test_loader, epsilon):\n","\n","  # Accuracy counter\n","  target_correct = other_correct_1 =other_correct_2 = 0\n","  valid_examples = valid_examples_1 = valid_examples_2 = 0\n","  adv_examples = []\n","\n","  # Loop over all examples in test set\n","  for data, target in test_loader:\n","    # Send the data and label to the device\n","    data, target = data.to(device), target.to(device)\n","\n","    # Set requires_grad attribute of tensor. Important for Attack\n","    data.requires_grad = True\n","\n","    # Forward pass the data through the model\n","    output = target_model(data)\n","    other_output_1 = other_model_1(data)\n","    other_output_2 = other_model_2(data)\n","    init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","    other_init_pred_1 = other_output_1.max(1, keepdim=True)[1]\n","    other_init_pred_2 = other_output_2.max(1, keepdim=True)[1]\n","\n","    # Model 1\n","    # If the initial prediction is wrong, dont bother attacking, just move on\n","    if init_pred.item() != target.item():\n","      continue\n","    else:\n","      valid_examples += 1\n","      # Calculate the loss\n","      loss = F.nll_loss(output, target)\n","      # Zero all existing gradients\n","      target_model.zero_grad()\n","      # Calculate gradients of model in backward pass\n","      loss.backward()\n","      # Collect datagrad\n","      data_grad = data.grad.data\n","      # Call FGSM Attack\n","      perturbed_data = fgsm_attack(data, epsilon, data_grad)\n","      # Re-classify the perturbed image\n","      output = target_model(perturbed_data)\n","\n","      # Check for success\n","      final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","      if final_pred.item() == target.item():\n","        target_correct += 1\n","        # Special case for saving 0 epsilon examples\n","        if (epsilon == 0) and (len(adv_examples) < 5):\n","          adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n","          adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n","        else:\n","          # Save some adv examples for visualization later\n","          if len(adv_examples) < 5:\n","            adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n","            adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n","\n","    # Model 2\n","    if other_init_pred_1.item() != target.item():\n","      pass\n","    else:\n","      valid_examples_1 += 1\n","      # Re-classify the perturbed image\n","      other_output_1 = other_model_1(perturbed_data)\n","\n","      # Check for success\n","      final_pred = other_output_1.max(1, keepdim=True)[1] # get the index of the max log-probability\n","      if final_pred.item() == target.item():\n","        other_correct_1 += 1\n","\n","\n","\n","    # Model 3\n","    if other_init_pred_2.item() != target.item():\n","      pass\n","    else:\n","      valid_examples_2 += 1\n","      # Re-classify the perturbed image\n","      other_output_2 = other_model_2(perturbed_data)\n","\n","      # Check for success\n","      final_pred = other_output_2.max(1, keepdim=True)[1] # get the index of the max log-probability\n","      if final_pred.item() == target.item():\n","        other_correct_2 += 1\n","   \n","\n","  # Calculate final accuracy for this epsilon\n","  final_acc = []\n","  final_acc_1 = target_correct/float(valid_examples)\n","  final_acc_2 = other_correct_1/float(valid_examples_1)\n","  final_acc_3 = other_correct_2/float(valid_examples_2)\n","    \n","  final_acc.append(final_acc_1)\n","  final_acc.append(final_acc_2)\n","  final_acc.append(final_acc_3)\n","\n","  print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, target_correct, valid_examples, final_acc_1))\n","  print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, other_correct_1, valid_examples_1, final_acc_2))\n","  print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, other_correct_2, valid_examples_2, final_acc_3))\n","\n","  # Return the accuracy and an adversarial example\n","  return final_acc, adv_examples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_xAz6x5UoEX"},"source":["adv_test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ru4HGUcB4OXc"},"source":["examples = []\n","accuracies = []\n","epsilons = [0.25]\n","# Test VGG16\n","for eps in epsilons:\n","    acc, ex = test(vgg16, resnet50, inception_v3, device, adv_test_loader, eps)\n","    accuracies.append(acc)\n","    examples.append(ex)"],"execution_count":null,"outputs":[]}]}